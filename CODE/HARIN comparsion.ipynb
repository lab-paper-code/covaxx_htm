{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-22 13:48:47.519098: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-11-22 13:48:48.706014: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from bertopic import BERTopic\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from gensim.test.utils import common_corpus, common_dictionary\n",
    "from gensim.models import HdpModel\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from gensim.models import Word2Vec\n",
    "from hdbscan import HDBSCAN\n",
    "from umap import UMAP\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy.spatial.distance import squareform\n",
    "from scipy.cluster import hierarchy as sch\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import tomotopy as tp\n",
    "from operator import itemgetter\n",
    "import operator\n",
    "from gensim.models import KeyedVectors\n",
    "import warnings\n",
    "import pickle\n",
    "from collections import Counter\n",
    "import os\n",
    "warnings.filterwarnings('ignore')\n",
    "import torch\n",
    "from itertools import product\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enter the dataset\n",
    "dataset_name = '20'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words_file = '../Datasets/stopwords_en.txt'\n",
    "stop_words =  [line.strip() for line in open(stop_words_file, encoding=\"utf-8\").readlines()]\n",
    "\n",
    "# For covid dataset\n",
    "# stop_words_file = '../Datasets/stopwords_kor.txt'\n",
    "# stop_words =  [line.strip() for line in open(stop_words_file, encoding=\"utf-8\").readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_feather(f'../Datasets/{dataset_name}/{dataset_name}.ftr')\n",
    "texts =data.words\n",
    "dictionary = Dictionary(texts)\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "docs = data.document.to_list()\n",
    "\n",
    "##For covid dataset\n",
    "# data = pd.read_feather(f'../Datasets/{dataset_name}/{dataset_name}.ftr')\n",
    "# texts =data.okt\n",
    "# dictionary = Dictionary(texts)\n",
    "# corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "# docs = data.corrected_twit.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Word2Vec for embedding model\n",
    "embedding_model = Word2Vec.load(f'../models/{dataset_name}/word2vec.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTokenizer:\n",
    "    def __init__(self, tagger):\n",
    "        self.tagger = tagger\n",
    "    def __call__(self, text):\n",
    "        text = re.sub(r'[^a-zA-Z\\s]', '', text.lower())\n",
    "        # Tokenize the text\n",
    "        tokens = text.split()\n",
    "        tokens = [self.tagger.stem(token) for token in tokens if token not in stop_words]\n",
    "        \n",
    "        return tokens\n",
    "\n",
    "## For covid dataset\n",
    "# class CustomTokenizer:\n",
    "#     def __init__(self, tagger):\n",
    "#         self.tagger = tagger\n",
    "#     def __call__(self, sent):\n",
    "#         #sent = sent[:1000000]\n",
    "#         hangul = re.compile('[^ 0-9가-힣+]')\n",
    "#         sent = hangul.sub(' ', sent)\n",
    "#         sent = \" \".join(sent.split())\n",
    "#         word_tokens = self.tagger.pos(sent, stem=True)\n",
    "#         temp = [word[0] for word in word_tokens if (word[1] =='Adjective' or  word[1] =='Noun')]\n",
    "#         result = [word for word in temp if (len(word) > 1  and ( not word in stop_words))]\n",
    "\n",
    "#         return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_topic(doc, topics):\n",
    "    # topic dictionary\n",
    "    topic_probabilities = {}\n",
    "\n",
    "    # for the topic\n",
    "    for topic_id, topic_words in topics.items():\n",
    "        # save the priority\n",
    "        topic_word_priority = {word: priority for priority, word in enumerate(reversed(topic_words))}\n",
    "\n",
    "        # extract the words in current topic\n",
    "        doc_words_in_topic = [word for word in doc if word in topic_word_priority]\n",
    "\n",
    "        # Calculate the sum of the priorities for each word\n",
    "        priority_sum = sum(topic_word_priority[word] for word in doc_words_in_topic)\n",
    "    \n",
    "        # Store the probability of belonging to a topic in a dictionary\n",
    "        topic_probabilities[topic_id] = priority_sum\n",
    "    \n",
    "    #  Choose topics with the highest probability\n",
    "    most_probable_topic = max(topic_probabilities.items(), key=operator.itemgetter(1))[0]\n",
    "    return most_probable_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_embedding_based_similarity(topic_dict, topic1, topic2):\n",
    "    '''similarity = 0\n",
    "    cnt=0 \n",
    "    for i in topic_dict.get(topic1)[:10]:\n",
    "        for j in topic_dict.get(topic2)[:10]:\n",
    "            try:\n",
    "                s = embedding_model.wv.similarity(i, j)\n",
    "                similarity += s\n",
    "                cnt+=1\n",
    "            except KeyError:\n",
    "                pass'''\n",
    "    s=0\n",
    "    topic_a = topic_dict.get(topic1)\n",
    "    topic_b = topic_dict.get(topic2)\n",
    "    for j in range(len(topic_a)):\n",
    "        for k in range(len(topic_b)):\n",
    "            try:\n",
    "                s+=embedding_model.wv.similarity(topic_a[j], topic_b[k])\n",
    "            except KeyError:\n",
    "                continue\n",
    "            \n",
    "    return s/(len(topic_a)*(len(topic_a)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## similarity\n",
    "def get_similarity(topic_dict, topn_dict):\n",
    "    similarity = []\n",
    "    for parents in topn_dict:\n",
    "        for child in topn_dict.get(parents):\n",
    "            if sum(x == y for x, y in zip(topic_dict.get(parents),topic_dict.get(child))) >= len(topic_dict.get(parents)) / 2:  \n",
    "                   similarity.append(get_word_embedding_based_similarity(topic_dict, parents, child)/2)\n",
    "            else: similarity.append(get_word_embedding_based_similarity(topic_dict, parents, child))\n",
    "          \n",
    "    else: return sum(similarity)/len(similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## diversity\n",
    "def get_diversity(topic_dict, topn_dict):\n",
    "    \n",
    "    diversity=[]\n",
    "    for i in topn_dict: \n",
    "        if len(topn_dict.get(i)) ==1 : continue\n",
    "        else: \n",
    "            d_list = topn_dict.get(i)\n",
    "            for j in range(len(d_list)):\n",
    "                for k in range(len(d_list)):\n",
    "                    if j!=k:diversity.append(1 - get_word_embedding_based_similarity(topic_dict, d_list[j], d_list[k]))\n",
    "               \n",
    "        \n",
    "\n",
    "    return sum(diversity) / len(diversity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## coherence\n",
    "def get_coherence(topic_dict, topn_dict):\n",
    "    # Make empty set for all values\n",
    "    unique_topics = set()\n",
    "\n",
    "    # Traverse all keys and values in the dictionary to remove duplicates and add unique int values to the set\n",
    "    for key, value in topn_dict.items():\n",
    "        unique_topics.update([key] + value)\n",
    "\n",
    "    topics = []\n",
    "    for i in unique_topics:\n",
    "        topics.append(topic_dict.get(i))\n",
    "    cm = CoherenceModel(topics = topics,\n",
    "                    texts =texts,\n",
    "                    dictionary = dictionary,\n",
    "                   coherence = 'c_npmi')\n",
    "    coherence = cm.get_coherence()\n",
    "    coherence = (coherence +1)/2\n",
    "    return coherence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## HARIN metric\n",
    "def HARIN(topic_dict,topn_dict):\n",
    "    s = get_similarity(topic_dict, topn_dict)\n",
    "    d = get_diversity(topic_dict, topn_dict)\n",
    "    c = get_coherence(topic_dict, topn_dict)\n",
    "    return s, d, c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is for the bertopic\n",
    "def get_hierarchy(model, k):\n",
    "    distance_function = lambda x: 1 - cosine_similarity(x)\n",
    "    linkage_function = lambda x: sch.linkage(x, 'ward', optimal_ordering=True)\n",
    "\n",
    "    # Calculate distance\n",
    "    embeddings = model.c_tf_idf_[model._outliers:k+1]\n",
    "    X = distance_function(embeddings)\n",
    "\n",
    "    # Make sure it is the 1-D condensed distance matrix with zeros on the diagonal\n",
    "    np.fill_diagonal(X, 0)\n",
    "    X = squareform(X)\n",
    "\n",
    "    # Use the 1-D condensed distance matrix as an input instead of the raw distance matrix\n",
    "    Z = linkage_function(X)\n",
    "\n",
    "    # Calculate basic bag-of-words to be iteratively merged later\n",
    "    documents = pd.DataFrame({\"Document\": docs,\n",
    "                              \"ID\": range(len(docs)),\n",
    "                              \"Topic\": model.topics_})\n",
    "    documents_per_topic = documents.groupby(['Topic'], as_index=False).agg({'Document': ' '.join})\n",
    "    documents_per_topic.loc[documents_per_topic.Topic != -1, :]\n",
    "    ## top 10 topics\n",
    "    documents_per_topic = documents_per_topic.loc[:k+1]\n",
    "    clean_documents = documents_per_topic.Document.values\n",
    "\n",
    "    # Scikit-Learn Deprecation: get_feature_names is deprecated in 1.0\n",
    "    # and will be removed in 1.2. Please use get_feature_names_out instead.\n",
    "    #if version.parse(sklearn_version) >= version.parse(\"1.0.0\"):\n",
    "    #words = model.vectorizer_model.get_feature_names_out()\n",
    "    #else:\n",
    "    words = model.vectorizer_model.get_feature_names_out()\n",
    "\n",
    "    bow = model.vectorizer_model.transform(clean_documents)\n",
    "\n",
    "    # Extract clusters\n",
    "    hier_topics = pd.DataFrame(columns=[\"Parent_ID\", \"Parent_Name\", \"Topics\",\n",
    "                                        \"Child_Left_ID\", \"Child_Left_Name\",\n",
    "                                        \"Child_Right_ID\", \"Child_Right_Name\"])\n",
    "    for index in (range(len(Z))):\n",
    "\n",
    "        # Find clustered documents\n",
    "        clusters = sch.fcluster(Z, t=Z[index][2], criterion='distance') - model._outliers\n",
    "        cluster_df = pd.DataFrame({\"Topic\": range(len(clusters)), \"Cluster\": clusters})\n",
    "        cluster_df = cluster_df.groupby(\"Cluster\").agg({'Topic': lambda x: list(x)}).reset_index()\n",
    "        #cluster_df = cluster_df.loc[:10]\n",
    "        nr_clusters = len(clusters)\n",
    "\n",
    "        # Extract first topic we find to get the s aet of topics in a merged topic\n",
    "        topic = None\n",
    "        val = Z[index][0]\n",
    "        while topic is None:\n",
    "            if val - len(clusters) < 0:\n",
    "                topic = int(val)\n",
    "            else:\n",
    "                val = Z[int(val - len(clusters))][0]\n",
    "        clustered_topics = [i for i, x in enumerate(clusters) if x == clusters[topic]]\n",
    "\n",
    "        # Group bow per cluster, calculate c-TF-IDF and extract words\n",
    "        grouped = csr_matrix(bow[clustered_topics].sum(axis=0))\n",
    "        c_tf_idf = model.ctfidf_model.fit_transform(grouped)\n",
    "        selection = documents.loc[documents.Topic.isin(clustered_topics), :]\n",
    "        selection.Topic = 0\n",
    "        words_per_topic = model._extract_words_per_topic(words, selection, c_tf_idf)\n",
    "\n",
    "        # Extract parent's name and ID\n",
    "        parent_id = index + len(clusters)\n",
    "        parent_name = \"_\".join([x[0] for x in words_per_topic[0]][:20])\n",
    "\n",
    "        # Extract child's name and ID\n",
    "        Z_id = Z[index][0]\n",
    "        child_left_id = Z_id if Z_id - nr_clusters < 0 else Z_id - nr_clusters\n",
    "\n",
    "        if Z_id - nr_clusters < 0:\n",
    "            child_left_name = \"_\".join([x[0] for x in model.get_topic(Z_id)][:20])\n",
    "        else:\n",
    "            child_left_name = hier_topics.iloc[int(child_left_id)].Parent_Name\n",
    "\n",
    "        # Extract child's name and ID\n",
    "        Z_id = Z[index][1]\n",
    "        child_right_id = Z_id if Z_id - nr_clusters < 0 else Z_id - nr_clusters\n",
    "\n",
    "        if Z_id - nr_clusters < 0:\n",
    "            child_right_name = \"_\".join([x[0] for x in model.get_topic(Z_id)][:20])\n",
    "        else:\n",
    "            child_right_name = hier_topics.iloc[int(child_right_id)].Parent_Name\n",
    "\n",
    "        # Save results\n",
    "        hier_topics.loc[len(hier_topics), :] = [parent_id, parent_name,\n",
    "                                                clustered_topics,\n",
    "                                                int(Z[index][0]), child_left_name,\n",
    "                                                int(Z[index][1]), child_right_name]\n",
    "\n",
    "    hier_topics[\"Distance\"] = Z[:, 2]\n",
    "    hier_topics = hier_topics.sort_values(\"Parent_ID\", ascending=False)\n",
    "    hier_topics[[\"Parent_ID\", \"Child_Left_ID\", \"Child_Right_ID\"]] = hier_topics[[\"Parent_ID\", \"Child_Left_ID\", \"Child_Right_ID\"]].astype(str)\n",
    "    return hier_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_parent(dictionary, value):\n",
    "    for key, values in dictionary.items():\n",
    "        if value in values:\n",
    "            return key\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## bertopic\n",
    "def get_bertopics(topk):\n",
    "    umap_model = UMAP(n_neighbors=15, n_components=5, \n",
    "          min_dist=0.0, metric='cosine', random_state=33)\n",
    "    custom_tokenizer = CustomTokenizer(PorterStemmer())\n",
    "    vectorizer = CountVectorizer(tokenizer = custom_tokenizer)\n",
    "    model = BERTopic(\n",
    "     top_n_words=30,\n",
    "     umap_model = umap_model,\n",
    "     vectorizer_model = vectorizer,\n",
    "     ).load(f'../models/{dataset_name}/bertopic')\n",
    "\n",
    "\n",
    "    hier_topics = get_hierarchy(model,topk)\n",
    "    parent_dict = {}\n",
    "    topic_dict = {}\n",
    "\n",
    "    # Start from the first row in hier_topics\n",
    "    root = hier_topics.iloc[0]['Parent_ID']\n",
    "    lid = hier_topics.iloc[0]['Child_Left_ID']\n",
    "    rid = hier_topics.iloc[0]['Child_Right_ID']\n",
    "    parent_dict[int(root)] = [int(lid), int(rid)]\n",
    "\n",
    "    if len(hier_topics[hier_topics['Parent_ID']==root]['Parent_Name']) > 0:\n",
    "        topic_words = hier_topics[hier_topics['Parent_ID']==root]['Parent_Name'].values[0].split('_')\n",
    "    else:\n",
    "        topic_words = [word[0] for word in model.get_topic(int(root))]\n",
    "    topic_dict[int(root)] = topic_words\n",
    "\n",
    "    if len(hier_topics[hier_topics['Parent_ID']==lid]['Parent_Name']) > 0:\n",
    "        topic_words = hier_topics[hier_topics['Parent_ID']==lid]['Parent_Name'].values[0].split('_')\n",
    "    else:\n",
    "        topic_words = [word[0] for word in model.get_topic(int(lid))]\n",
    "    topic_dict[int(lid)] = topic_words\n",
    "\n",
    "    if len(hier_topics[hier_topics['Parent_ID']==rid]['Parent_Name']) > 0:\n",
    "        topic_words = hier_topics[hier_topics['Parent_ID']==rid]['Parent_Name'].values[0].split('_')\n",
    "    else:\n",
    "        topic_words = [word[0] for word in model.get_topic(int(rid))]\n",
    "    topic_dict[int(rid)] = topic_words\n",
    "\n",
    "    llid = hier_topics[hier_topics['Parent_ID']==lid]['Child_Left_ID'].values[0]\n",
    "    lrid =  hier_topics[hier_topics['Parent_ID']==lid]['Child_Right_ID'].values[0]\n",
    "    parent_dict[int(lid)] = [int(llid), int(lrid)]\n",
    "\n",
    "    if len(hier_topics[hier_topics['Parent_ID']==llid]['Parent_Name']) > 0:\n",
    "        topic_words = hier_topics[hier_topics['Parent_ID']==llid]['Parent_Name'].values[0].split('_')\n",
    "    else:\n",
    "        topic_words = [word[0] for word in model.get_topic(int(llid))]\n",
    "    topic_dict[int(llid)] = topic_words\n",
    "\n",
    "    if len(hier_topics[hier_topics['Parent_ID']==lrid]['Parent_Name']) > 0:\n",
    "        topic_words = hier_topics[hier_topics['Parent_ID']==lrid]['Parent_Name'].values[0].split('_')\n",
    "    else:\n",
    "        topic_words = [word[0] for word in model.get_topic(int(lrid))]\n",
    "    topic_dict[int(lrid)] = topic_words\n",
    "\n",
    "    rlid = hier_topics[hier_topics['Parent_ID']==rid]['Child_Left_ID'].values[0]\n",
    "    rrid =  hier_topics[hier_topics['Parent_ID']==rid]['Child_Right_ID'].values[0]\n",
    "    parent_dict[int(rid)] = [int(rlid), int(rrid)]\n",
    "\n",
    "    if len(hier_topics[hier_topics['Parent_ID']==rlid]['Parent_Name']) > 0:\n",
    "        topic_words = hier_topics[hier_topics['Parent_ID']==rlid]['Parent_Name'].values[0].split('_')\n",
    "    else:\n",
    "        topic_words = [word[0] for word in model.get_topic(int(rlid))]\n",
    "    topic_dict[int(rlid)] = topic_words\n",
    "\n",
    "    if len(hier_topics[hier_topics['Parent_ID']==rrid]['Parent_Name']) > 0:\n",
    "        topic_words = hier_topics[hier_topics['Parent_ID']==rrid]['Parent_Name'].values[0].split('_')\n",
    "    else:\n",
    "        topic_words = [word[0] for word in model.get_topic(int(rrid))]\n",
    "    topic_dict[int(rrid)] = topic_words\n",
    "\n",
    "    topn_dict = parent_dict\n",
    "    \n",
    "    return topic_dict,topn_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## hlda\n",
    "def get_hldas(topk):\n",
    "    model = tp.HLDAModel().load(f'../models/{dataset_name}/hlda.bin')\n",
    "    parent_dict = {}\n",
    "    topic_dict = {}\n",
    "    parent_dict[0] = list(model.children_topics(0))\n",
    "    topic_dict[0] = [words[0] for words in model.get_topic_words(0,top_n=20)]\n",
    "    for depth1 in model.children_topics(0):\n",
    "        parent_dict[depth1] = list(model.children_topics(depth1))\n",
    "        topic_dict[depth1] = [words[0] for words in model.get_topic_words(depth1,top_n=20)]\n",
    "        for topic in model.children_topics(depth1):\n",
    "            topic_dict[topic] = [words[0] for words in model.get_topic_words(topic,top_n=20)]\n",
    "                                                      \n",
    "            \n",
    "    topic_numdoc = {}\n",
    "    total =0\n",
    "    for depth1 in model.children_topics(0):\n",
    "        for topic in model.children_topics(depth1):\n",
    "            total +=model.num_docs_of_topic(topic)\n",
    "            topic_numdoc[topic] = model.num_docs_of_topic(topic)\n",
    "\n",
    "\n",
    "    res = dict(sorted(topic_numdoc.items(), key=itemgetter(1), reverse=True)[:topk])\n",
    "    topn_dict = {}\n",
    "    for child in res:\n",
    "        if model.parent_topic(child) in topn_dict:\n",
    "            temp = topn_dict.get(model.parent_topic(child))\n",
    "            temp.append(child)\n",
    "            topn_dict[model.parent_topic(child)] = temp\n",
    "\n",
    "        else : topn_dict[model.parent_topic(child)] = [child]  \n",
    "\n",
    "    temp = list(topn_dict.keys())\n",
    "    topn_dict[0] = temp\n",
    "    \n",
    "    return topic_dict, topn_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cluhtm_topics(line):\n",
    "    line=line.replace('\\'','')\n",
    "    line=line.split(\"[\")[1]\n",
    "    line=line.split(\"]\")[0]\n",
    "    return line.split(\", \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "## cluhtm\n",
    "def get_cluhtms(topk):\n",
    "    file=open(f'../models/{dataset_name}/hierarchical_structure_cluhtm.txt', 'r')\n",
    "    readdata = file.readlines()\n",
    "    raw = pd.read_feather(f'../Datasets/{dataset_name}/{dataset_name}.ftr')\n",
    "    raw['label'] = range(len(raw))\n",
    "\n",
    "    num_depth0 = 0\n",
    "    num_depth1 = 0\n",
    "    num_depth2 = 0\n",
    "    for line in readdata:\n",
    "        if '\\t' not in line: num_depth0+=1\n",
    "        elif '\\t\\t'not in line: num_depth1+=1\n",
    "        else: num_depth2+=1\n",
    "\n",
    "    num_depth1 += num_depth0\n",
    "    num_depth2 += num_depth1\n",
    "\n",
    "    topic_dict={}\n",
    "    parent_dict={}\n",
    "    leaf_topics={}\n",
    "\n",
    "    for line in readdata:\n",
    "        if '\\t' not in line:\n",
    "            num_depth0 -=1\n",
    "            topic_dict[num_depth0] = get_cluhtm_topics(line)\n",
    "\n",
    "        elif '\\t\\t'not in line:\n",
    "            num_depth1 -=1\n",
    "            topic_dict[num_depth1] = get_cluhtm_topics(line)\n",
    "            if parent_dict.get(num_depth0) is None :\n",
    "                parent_dict[num_depth0] = [num_depth1]\n",
    "            else :\n",
    "                temp = parent_dict.get(num_depth0)\n",
    "                temp.append(num_depth1)\n",
    "                parent_dict[num_depth0] = temp    \n",
    "        else:\n",
    "            num_depth2 -=1\n",
    "            leaf_topics[num_depth2] = get_cluhtm_topics(line)\n",
    "            topic_dict[num_depth2] = get_cluhtm_topics(line)\n",
    "            if parent_dict.get(num_depth1) is None :\n",
    "                parent_dict[num_depth1] = [num_depth2]\n",
    "            else :\n",
    "                temp = parent_dict.get(num_depth1)\n",
    "                temp.append(num_depth2)\n",
    "                parent_dict[num_depth1] = temp\n",
    "        \n",
    "        \n",
    "    for i in range(len(raw)):\n",
    "        raw.label[i] = assign_topic(raw.words[i],leaf_topics)\n",
    "            \n",
    "        \n",
    "    res = raw['label'].value_counts()[:topk].index\n",
    "    topn_dict = {}\n",
    "    for child in res:\n",
    "        if find_parent(parent_dict, child) in topn_dict:\n",
    "            temp = topn_dict.get(find_parent(parent_dict, child))\n",
    "            temp.append(child)\n",
    "            topn_dict[find_parent(parent_dict, child)] = temp\n",
    "        else : topn_dict[find_parent(parent_dict, child)] = [child] \n",
    "\n",
    "    for i in list(topn_dict.keys()):    \n",
    "        if find_parent(parent_dict, i) in topn_dict:\n",
    "            temp = topn_dict.get(find_parent(parent_dict, i))\n",
    "            temp.append(i)\n",
    "            topn_dict[find_parent(parent_dict, i)] = temp\n",
    "\n",
    "        else : topn_dict[find_parent(parent_dict, i)] = [i]  \n",
    "        \n",
    "\n",
    "    return topic_dict, topn_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hyhtm_topics(line):\n",
    "    line=line.replace('\\'','')\n",
    "    line=line.replace('\\t','')\n",
    "    line=line.replace('\\n','')\n",
    "    return line.split(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "## hyhtm\n",
    "def get_hyhtms(topk):\n",
    "    file=open(f'../models/{dataset_name}/hierarchical_structure_hyhtm.txt', 'r')\n",
    "    readdata = file.readlines()\n",
    "    raw = pd.read_feather(f'../Datasets/{dataset_name}/{dataset_name}.ftr')\n",
    "    raw['label'] = range(len(raw))\n",
    "\n",
    "    num_depth0 = 0\n",
    "    num_depth1 = 0\n",
    "    num_depth2 = 0\n",
    "    for line in readdata:\n",
    "        if '\\t' not in line: num_depth0+=1\n",
    "        elif '\\t\\t'not in line: num_depth1+=1\n",
    "        else: num_depth2+=1\n",
    "\n",
    "    num_depth1 += num_depth0\n",
    "    num_depth2 += num_depth1\n",
    "\n",
    "    topic_dict={}\n",
    "    parent_dict={}\n",
    "    leaf_topics={}\n",
    "\n",
    "    for line in readdata:\n",
    "        # depth0 : L0 topic(root topic)\n",
    "        if '\\t' not in line:\n",
    "            num_depth0 -=1\n",
    "            topic_dict[num_depth0] = get_hyhtm_topics(line)\n",
    "\n",
    "        # depth1 : L1 topic(parent topic)\n",
    "        elif '\\t\\t'not in line:\n",
    "            num_depth1 -=1\n",
    "            topic_dict[num_depth1] = get_hyhtm_topics(line)\n",
    "            if parent_dict.get(num_depth0) is None :\n",
    "                parent_dict[num_depth0] = [num_depth1]\n",
    "            else :\n",
    "                temp = parent_dict.get(num_depth0)\n",
    "                temp.append(num_depth1)\n",
    "                parent_dict[num_depth0] = temp\n",
    "\n",
    "        # depth2 : L2 topic(child topic)\n",
    "        else:\n",
    "            num_depth2 -=1\n",
    "            leaf_topics[num_depth2] = get_hyhtm_topics(line)\n",
    "            topic_dict[num_depth2] = get_hyhtm_topics(line)\n",
    "            if parent_dict.get(num_depth1) is None :\n",
    "                parent_dict[num_depth1] = [num_depth2]\n",
    "            else :\n",
    "                temp = parent_dict.get(num_depth1)\n",
    "                temp.append(num_depth2)\n",
    "                parent_dict[num_depth1] = temp\n",
    "\n",
    "    for i in range(len(raw)):\n",
    "        raw.label[i] = assign_topic(raw.words[i],leaf_topics)\n",
    "\n",
    "\n",
    "    res = raw['label'].value_counts()[:topk].index\n",
    "    topn_dict = {}\n",
    "    for child in res:\n",
    "        if find_parent(parent_dict, child) in topn_dict:\n",
    "            temp = topn_dict.get(find_parent(parent_dict, child))\n",
    "            temp.append(child)\n",
    "            topn_dict[find_parent(parent_dict, child)] = temp\n",
    "        else : topn_dict[find_parent(parent_dict, child)] = [child] \n",
    "\n",
    "    for i in list(topn_dict.keys()):    \n",
    "        if find_parent(parent_dict, i) in topn_dict:\n",
    "            temp = topn_dict.get(find_parent(parent_dict, i))\n",
    "            temp.append(i)\n",
    "            topn_dict[find_parent(parent_dict, i)] = temp\n",
    "\n",
    "        else : topn_dict[find_parent(parent_dict, i)] = [i]  \n",
    "\n",
    "    return topic_dict, topn_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bertopic\n",
      "cluhtm\n",
      "hlda\n",
      "hyhtm\n"
     ]
    }
   ],
   "source": [
    "## Calculate the HARIN score\n",
    "score_df = pd.DataFrame(columns=['model','k', 's', 'd', 'c','s+d','s+c','c+d','HARIN'])\n",
    "models = ['bertopic', 'cluhtm', 'hlda', 'hyhtm']\n",
    "output = []\n",
    "for model_name in models:\n",
    "    print(model_name)\n",
    "    for topk in range(10,60,10):\n",
    "        if model_name =='bertopic':\n",
    "            topic_dict, topn_dict = get_bertopics(topk)\n",
    "        elif model_name =='cluhtm':\n",
    "            topic_dict, topn_dict = get_cluhtms(topk)\n",
    "        if model_name =='hlda':\n",
    "            topic_dict, topn_dict = get_hldas(topk)\n",
    "        elif model_name =='hyhtm':\n",
    "            topic_dict, topn_dict = get_hyhtms(topk)    \n",
    "        \n",
    "        \n",
    "      # Calculate the baseline metrics\n",
    "        s, d, c= HARIN(topic_dict, topn_dict)\n",
    "        output.append({'model': model_name, 'k': topk, 's' : s, 'd' : d, 'c' : c, 's+d' : s+d, 's+c' : s+c, 'c+d' : c+d, 's+c+d': s+c+d, 'HARIN': (0.33 * s) + (0.33 * d)+ (0.33 * c)})\n",
    "        \n",
    "score_df = pd.DataFrame(output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HTM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
